{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be60341-fe75-47c1-b33c-d7bacb1fe4ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this notebook, you will craft sophisticated ETL jobs that interface with a variety of common data sources, such as \n",
    "- REST APIs (HTTP endpoints)\n",
    "- RDBMS\n",
    "- Hive tables (managed tables)\n",
    "- Various file formats (csv, json, parquet, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9fe8dc-6b2e-4499-8961-7e01309d05f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d\n",
    "\n",
    "# Interview Questions\n",
    "\n",
    "As you progress through the practice, attempt to answer the following questions:\n",
    "\n",
    "## Columnar File\n",
    "- What is a columnar file format and what advantages does it offer?\n",
    "- Why is Parquet frequently used with Spark and how does it function?\n",
    "- How do you read/write data from/to a Parquet file using a DataFrame?\n",
    "\n",
    "## Partitions\n",
    "- How do you save data to a file system by partitions? (Hint: Provide the code)\n",
    "- How and why can partitions reduce query execution time? (Hint: Give an example)\n",
    "\n",
    "## JDBC and RDBMS\n",
    "- How do you load data from an RDBMS into Spark? (Hint: Discuss the steps and JDBC)\n",
    "\n",
    "## REST API and HTTP Requests\n",
    "- How can Spark be used to fetch data from a REST API? (Hint: Discuss making API requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c7f0dcb-2214-41ae-a6f4-12d5a34506ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job One: Parquet file\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Data transformation requirements https://pgexercises.com/questions/aggregates/fachoursbymonth.html\n",
    "\n",
    "### Load\n",
    "Load data into a parquet file\n",
    "\n",
    "### What is Parquet? \n",
    "\n",
    "Columnar files are an important technique for optimizing Spark queries. Additionally, they are often tested in interviews.\n",
    "- https://www.youtube.com/watch?v=KLFadWdomyI\n",
    "- https://www.databricks.com/glossary/what-is-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33324d02-bc67-4c31-b822-8fe8c69fead5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n|facid|Total Slots|\n+-----+-----------+\n|    5|        122|\n|    3|        422|\n|    7|        426|\n|    8|        471|\n|    6|        540|\n|    2|        570|\n|    1|        588|\n|    0|        591|\n|    4|        648|\n+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write your solution here\n",
    "# Extract data from managed tables\n",
    "\n",
    "bookings_df = spark.sql(\"select * from bookings\")\n",
    "members_df = spark.sql(\"select * from members\")\n",
    "facilities_df = spark.sql(\"select * from facilities\")\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Transform: Filter, group, and aggregate data\n",
    "transformed_df = bookings_df.filter(\n",
    "    (col(\"starttime\") >= '2012-09-01') & (col(\"starttime\") < '2012-10-01')\n",
    ").groupBy(\"facid\").agg(\n",
    "    sum(\"slots\").alias(\"Total Slots\")\n",
    ").orderBy(\"Total Slots\")\n",
    "\n",
    "# Load data into a Parquet file\n",
    "output_path = \"/path/to/output/directory/total_slots.parquet\"\n",
    "\n",
    "transformed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e13381-1abd-4883-9e76-a5aeb8b04266",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n|facid|Total Slots|\n+-----+-----------+\n|    5|        122|\n|    3|        422|\n|    7|        426|\n|    8|        471|\n|    6|        540|\n|    2|        570|\n|    1|        588|\n|    0|        591|\n|    4|        648|\n+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/path/to/output/directory/total_slots.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b51d425e-d532-47e5-8cbf-a91ca78246b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Two: Partitions\n",
    "\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Transform the data https://pgexercises.com/questions/joins/threejoin.html\n",
    "\n",
    "### Load\n",
    "Partition the result data by facility column and then save to `threejoin_delta` managed table. Additionally, they are often tested in interviews.\n",
    "\n",
    "hint: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html\n",
    "\n",
    "What are paritions? \n",
    "\n",
    "Partitions are an important technique to optimize Spark queries\n",
    "- https://www.youtube.com/watch?v=hvF7tY2-L3U&t=268s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aea2ca-5178-4034-91ee-c09942c5f518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n|        member|      facility|\n+--------------+--------------+\n|    Anne Baker|Tennis Court 1|\n|    Anne Baker|Tennis Court 2|\n|  Burton Tracy|Tennis Court 1|\n|  Burton Tracy|Tennis Court 2|\n|  Charles Owen|Tennis Court 1|\n|  Charles Owen|Tennis Court 2|\n|  Darren Smith|Tennis Court 2|\n| David Farrell|Tennis Court 1|\n| David Farrell|Tennis Court 2|\n|   David Jones|Tennis Court 1|\n|   David Jones|Tennis Court 2|\n|  David Pinker|Tennis Court 1|\n| Douglas Jones|Tennis Court 1|\n| Erica Crumpet|Tennis Court 1|\n|Florence Bader|Tennis Court 1|\n|Florence Bader|Tennis Court 2|\n|   GUEST GUEST|Tennis Court 1|\n|   GUEST GUEST|Tennis Court 2|\n|Gerald Butters|Tennis Court 1|\n|Gerald Butters|Tennis Court 2|\n+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Write your solution here\n",
    "# Extract data from managed tables\n",
    "\n",
    "bookings_df = spark.sql(\"select * from bookings\")\n",
    "members_df = spark.sql(\"select * from members\")\n",
    "facilities_df = spark.sql(\"select * from facilities\")\n",
    "\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Join the tables and filter the required facilities\n",
    "transformed_df = members_df.join(bookings_df, members_df.memid == bookings_df.memid) \\\n",
    "    .join(facilities_df, bookings_df.facid == facilities_df.facid) \\\n",
    "    .filter(facilities_df.name.isin('Tennis Court 2', 'Tennis Court 1')) \\\n",
    "    .select(concat_ws(' ', members_df.firstname, members_df.surname).alias(\"member\"),\n",
    "            facilities_df.name.alias(\"facility\")) \\\n",
    "    .distinct() \\\n",
    "    .orderBy(\"member\", \"facility\")\n",
    "\n",
    "# Save the DataFrame as a managed table, partitioned by the 'facility' column\n",
    "transformed_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"facility\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"threejoin_delta\")\n",
    "\n",
    "transformed_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27653501-7f85-4a5e-bb37-71d7d01c8060",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|      facility|\n+--------------+\n|Tennis Court 2|\n|Tennis Court 1|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"SHOW PARTITIONS threejoin_delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7610de14-acd6-4374-945d-661dbc08a08e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Three: HTTP Requests\n",
    "\n",
    "### Extract\n",
    "Extract daily stock price data price from the following companies, Google, Apple, Microsoft, and Tesla. \n",
    "\n",
    "Data Source\n",
    "- API: https://rapidapi.com/alphavantage/api/alpha-vantage\n",
    "- Endpoint: GET `TIME_SERIES_DAILY`\n",
    "\n",
    "Sample HTTP request\n",
    "\n",
    "```\n",
    "curl --request GET \\\n",
    "\t--url 'https://alpha-vantage.p.rapidapi.com/query?function=TIME_SERIES_DAILY&symbol=TSLA&outputsize=compact&datatype=json' \\\n",
    "\t--header 'X-RapidAPI-Host: alpha-vantage.p.rapidapi.com' \\\n",
    "\t--header 'X-RapidAPI-Key: [YOUR_KEY]'\n",
    "\n",
    "```\n",
    "\n",
    "Sample Python HTTP request\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "querystring = {\n",
    "    \"function\":\"TIME_SERIES_DAILY\",\n",
    "    \"symbol\":\"IBM\",\n",
    "    \"datatype\":\"json\",\n",
    "    \"outputsize\":\"compact\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"[YOUR_KEY]\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Now 'data' contains the daily time series data for \"IBM\"\n",
    "```\n",
    "\n",
    "### Transform\n",
    "Find **weekly** max closing price for each company.\n",
    "\n",
    "hints: \n",
    "  - Use a `for-loop` to get stock data for each company\n",
    "  - Use the spark `union` operation to concat all data into one DF\n",
    "  - create a new `week` column from the data column\n",
    "  - use `group by` to calcualte max closing price\n",
    "\n",
    "### Load\n",
    "- Partition `DF` by company\n",
    "- Load the DF in to a managed table called, `max_closing_price_weekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acfc2feb-a7db-437c-a72b-a3dc3cb21997",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+\n|      date|company| close|\n+----------+-------+------+\n|2024-08-12|  GOOGL|162.29|\n|2024-08-09|  GOOGL|163.67|\n|2024-08-08|  GOOGL|162.03|\n|2024-08-07|  GOOGL|158.94|\n|2024-08-06|  GOOGL|158.29|\n|2024-08-05|  GOOGL|159.25|\n|2024-08-02|  GOOGL|166.66|\n|2024-08-01|  GOOGL|170.76|\n|2024-07-31|  GOOGL|171.54|\n|2024-07-30|  GOOGL|170.29|\n|2024-07-29|  GOOGL|169.53|\n|2024-07-26|  GOOGL| 167.0|\n|2024-07-25|  GOOGL|167.28|\n|2024-07-24|  GOOGL|172.63|\n|2024-07-23|  GOOGL|181.79|\n|2024-07-22|  GOOGL|181.67|\n|2024-07-19|  GOOGL|177.66|\n|2024-07-18|  GOOGL|177.69|\n|2024-07-17|  GOOGL|181.02|\n|2024-07-16|  GOOGL|183.92|\n+----------+-------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, weekofyear, max as max_\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StockPricesETL\").getOrCreate()\n",
    "\n",
    "# Define the API details\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"f344ede3famshcfbe67e26491b84p1d77cejsnd90a2a421393\"\n",
    "}\n",
    "\n",
    "# List of companies to fetch data for\n",
    "companies = [\"GOOGL\", \"AAPL\", \"MSFT\", \"TSLA\"]\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"company\", StringType(), True),\n",
    "    StructField(\"close\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Initialize an empty DataFrame with the defined schema\n",
    "combined_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# Loop through each company and extract data\n",
    "for company in companies:\n",
    "    querystring = {\n",
    "        \"function\": \"TIME_SERIES_DAILY\",\n",
    "        \"symbol\": company,\n",
    "        \"datatype\": \"json\",\n",
    "        \"outputsize\": \"compact\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the daily time series data\n",
    "    time_series = data.get(\"Time Series (Daily)\", {})\n",
    "\n",
    "    # Transform the data into a list of dictionaries\n",
    "    records = [\n",
    "        {\"date\": date, \"company\": company, \"close\": float(values[\"4. close\"])}\n",
    "        for date, values in time_series.items()\n",
    "    ]\n",
    "\n",
    "    # Convert the records into a DataFrame using the predefined schema\n",
    "    df = spark.createDataFrame(records, schema=schema)\n",
    "\n",
    "    # Union the data into a single DataFrame\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "# Convert the 'date' column to DateType\n",
    "combined_df = combined_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Display the combined DataFrame to ensure correctness\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1671e5f1-afae-44bb-b8ba-e3798d013b2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a 'week' column based on the date\n",
    "transformed_df = combined_df.withColumn(\"week\", weekofyear(col(\"date\")))\n",
    "\n",
    "# Group by company and week, then calculate the max closing price\n",
    "weekly_max_df = transformed_df.groupBy(\"company\", \"week\") \\\n",
    "    .agg(max_(\"close\").alias(\"max_closing_price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b189b78b-888e-45fd-a280-315c0e922c27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame to a managed table, partitioned by company\n",
    "weekly_max_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"company\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"max_closing_price_weekly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "892d4834-55be-4160-9b46-788efbf0301d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n|company|\n+-------+\n|   AAPL|\n|  GOOGL|\n|   MSFT|\n|   TSLA|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW PARTITIONS max_closing_price_weekly\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f98592-1f5f-4b42-9350-6720e69a7c22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Four: RDBMS\n",
    "\n",
    "\n",
    "### Extract\n",
    "Extract RNA data from a public PostgreSQL database.\n",
    "\n",
    "- https://rnacentral.org/help/public-database\n",
    "- Extract 100 RNA records from the `rna` table (hint: use `limit` in your sql)\n",
    "- hint: use `spark.read.jdbc` https://docs.databricks.com/external-data/jdbc.html\n",
    "\n",
    "### Transform\n",
    "We want to load the data as it so there is no transformation required.\n",
    "\n",
    "\n",
    "### Load\n",
    "Load the DF in to a managed table called, `rna_100_records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3011d775-d108-4cb0-85d1-bf21ae1c23d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"RNADataETL\").getOrCreate()\n",
    "\n",
    "# PostgreSQL connection properties\n",
    "jdbc_url = \"jdbc:postgresql://hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs\"\n",
    "connection_properties = {\n",
    "    \"user\": \"reader\",\n",
    "    \"password\": \"NWDMCE5xdipIjRrp\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# SQL query to extract 100 records from the rna table\n",
    "query = \"(SELECT * FROM rna LIMIT 100) AS rna_100\"\n",
    "\n",
    "# Read data from PostgreSQL database\n",
    "rna_df = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_properties)\n",
    "# Load the DataFrame into a managed table\n",
    "rna_df.write.mode(\"overwrite\").saveAsTable(\"rna_100_records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca1bf19-8d31-403b-813c-68f9d8b0cd56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------+---------+----------------+----+--------------------+--------+--------------------+\n|      id|          upi|           timestamp|userstamp|           crc64| len|           seq_short|seq_long|                 md5|\n+--------+-------------+--------------------+---------+----------------+----+--------------------+--------+--------------------+\n| 5110258|URS00004DF9F2| 2014-05-29 13:51:05|   RNACEN|61AE5251E4F4E67A| 848|GATAAACGCTAGCGGAG...|    null|26fa8b5b4fbc5e3bc...|\n| 5110259|URS00004DF9F3| 2014-05-29 13:51:05|   RNACEN|63056E024E222787|  76|GCGGGCGTAGCTCAGTT...|    null|f725b5fff34986f21...|\n| 5110262|URS00004DF9F6| 2014-05-29 13:51:05|   RNACEN|06115F73D962D669| 693|TGCAGTCGGACGGGATT...|    null|26fa93769ae137737...|\n| 5110264|URS00004DF9F8| 2014-05-29 13:51:05|   RNACEN|168BF325E98D59CD|1514|AGAGTTTGATCATGGCT...|    null|be74650d6063133a0...|\n| 5110266|URS00004DF9FA| 2014-05-29 13:51:05|   RNACEN|F7A8B3D48878A1EB|1415|GCGGCATGGATTAGGCA...|    null|26faa3be156cb690c...|\n| 5110267|URS00004DF9FB| 2014-05-29 13:51:05|   RNACEN|FCA8596BB5F98709|1396|GAGTTTGATCCTGGCTC...|    null|f725bf048ae9628b0...|\n| 5110268|URS00004DF9FC| 2014-05-29 13:51:05|   RNACEN|7BA613E91B05E199| 236|CGGGGGTGGAGGTAAAT...|    null|be746debf17c5ff84...|\n| 4948993|URS00004B8401| 2014-05-29 13:51:05|   RNACEN|56CBAF315D3D555B| 788|AGAGTTTGATCATGGCT...|    null|b46666031cd2f2f6f...|\n| 5110270|URS00004DF9FE| 2014-05-29 13:51:05|   RNACEN|C11DC4A7F716C0DF|1184|GAGTTTGATCCTGGCTC...|    null|26faad751908d2405...|\n| 5110271|URS00004DF9FF| 2014-05-29 13:51:05|   RNACEN|2E9B9B0ED6786546| 564|CGCCTCTTGAATTAATA...|    null|f725bfd30220cd9e6...|\n| 5110272|URS00004DFA00| 2014-05-29 13:51:05|   RNACEN|94007F6B4E7F7C9E| 547|TACGGGAGGCAGCAGTG...|    null|be748e7ba8a6e2ddb...|\n| 5110273|URS00004DFA01| 2014-05-29 13:51:05|   RNACEN|43A753EB097C3270| 961|GAGTTTGATCCTGCCCT...|    null|d0235eb70ccdf67f3...|\n| 5110277|URS00004DFA05| 2014-05-29 13:51:05|   RNACEN|3A489074684FDBBC| 648|TGTAGTAAGGCGTTAAG...|    null|d02382578ed28f924...|\n| 5110279|URS00004DFA07| 2014-05-29 13:51:05|   RNACEN|EFE28BCF5970F2CB| 488|CACGTAGTTAGCCAGTG...|    null|f725e420c95a5d461...|\n| 5110282|URS00004DFA0A| 2014-05-29 13:51:05|   RNACEN|94440DA66417C949| 229|TGAGTAACACGTATCCA...|    null|26fab35110496f809...|\n| 5110287|URS00004DFA0F| 2014-05-29 13:51:05|   RNACEN|43074E0B628AA862|1340|ATTCGATTAGAGTTTGA...|    null|f725f67b00428b3c4...|\n| 5110289|URS00004DFA11| 2014-05-29 13:51:05|   RNACEN|92D8FF6D4339313B| 474|AGGACGAACGCTGGCGG...|    null|d02392a7d0a2f279e...|\n| 4948996|URS00004B8404| 2014-05-29 13:51:05|   RNACEN|0C7670CCF244A51A| 112|AACCCTTTGTGAACCAT...|    null|5a61a80915c151441...|\n|12772836|URS0000C2E5E4|2017-10-19 09:46:...|   rnacen|E1E3699AC5E10EF6|  72|GGTTTCATAGTGTAGTG...|    null|50dabfb5b44709f71...|\n| 5110292|URS00004DFA14| 2014-05-29 13:51:05|   RNACEN|A99F45F6520090A9|1341|AGCGGAAGGCTTAATAC...|    null|be74be36fefb5a1ce...|\n+--------+-------------+--------------------+---------+----------------+----+--------------------+--------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM rna_100_records\")\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - Spark ETL Jobs Exercieses",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
